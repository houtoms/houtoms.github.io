---
layout: posts
title:  "Fused Nodes in Tensorflow"
author: kaixi_hou
#search                   : true
#search_full_content      : true
#search_provider          : google
#comments: true
---
(Draft, Work in Progress)
## Introduction
Why fused APIs? Some patterns are more frequent than others, between those ops inside, we need to store back and fetch again. To reduce access the slow offchip memory, 
Remaps them onto more efficient implementations by replacing commonly occuring subgraphs with optimized fused monolithic kernels.
There could be an considerable number of combinations of potential ops to be fused, bring up hardness for maintaining a well APIs. Even for the most popular pattens, new models could introduce new patterns. Therefore Fused APIs are usually not be dirrectly accessable by users. TF provides Grapper remapper to detect such patterns and remap them with a more efficient fused node which will call fused kernel.
This post will discuss the convlution related fusion patterns and discuss how to trigger the fussion and how they looks like.

Python settings:
```python
import os
os.environ['TF_CPP_VMODULE'] = 'remapper=2'

import tensorflow as tf
import numpy as np

use_nhwc = True

N, H, W, C = 1, 2, 3, 3
k, r, s, c = 3, 2, 2, C

if use_nhwc:
  x_format = 'NHWC'
  x_format_keras = 'channels_last'
  bias_format = 'N...C'
  x_shape = (N, H, W, C)
  channel_axis = -1
else:
  x_format = 'NCHW'
  x_format_keras = 'channels_first'
  bias_format = 'NC...'
  x_shape = (N, C, H, W)
  channel_axis = 1
```

## Fused Convolution
### Conv2D + BiasAdd + Relu
How remapper do the optimization?
Search from the root of the pattern, sweep from the reversed order of the topological sort. For each node, if it is root of some pattern, it will go back to check other ops in the pattern and go  thought a series of checkings on the device, dtype, layout, etc. If everything looks good, a fused node will be replaced with the nodes in the pattern.
we use a common pattern to illustrate this process conv2d biasadd relu. We first
use the lower api to do the test as below, which we have to prepare the parameters filters and biases ourselves.
```python
f_np = np.random.random([r, s, c, k]).astype(np.float32)
f = tf.Variable(f_np)
b_np = np.random.random([k]).astype(np.float32)
b = tf.Variable(b_np)

@tf.function
def fused_conv_bias_relu(x):
  y = tf.nn.conv2d(x, f, strides=(1,1), padding='SAME',
                   data_format=x_format)
  y = tf.nn.bias_add(y, b, data_format=bias_format)
  y = tf.nn.relu(y)
  return y
```
We need to use the tf.function to make it as a graph and then graph optimizers including the remapper will be applied.
Then, we use the following code to call the above function
```python
inputs = tf.random.normal(x_shape)
outputs = fused_conv_bias_relu(inputs)
print(outputs)
```

Here, we are more interested in this line in the output (logging actived with TFMODEL), which shows the pattern is correctly found and fusion is done.
```
I tensorflow/core/grappler/optimizers/remapper.cc:1114] Fuse Conv2D with BiasAdd and Relu: activation=Relu bias_add=BiasAdd contraction=Conv2D
```
I also output the unoptimized graph and optimized graph from tensorboard (using SerializeAsString() output as pb file). the figures are adapted from the output of tensorboard:

![Title](/assets/posts_images/unfused.png)
![Title](/assets/posts_images/fused.png)


We can see the relu node is the root and replaced with fused node. All the corresponding wires are changed as well that all the weights are to fused node and x to fused node as well.
Here is some note what requirements for current fusion to be triggered on GPU.
To use the high level api like keras:

```python
conv2d = tf.keras.layers.Conv2D(
             filters=k, kernel_size=(r, s), padding='same',
             data_format=x_format_keras)
relu = tf.keras.layers.ReLU()

@tf.function
def fused_conv_bias_relu_keras(x):
  y = conv2d(x)
  y = relu(y)
  return y
```
Note, Support NHWC + fp32 (=NCHW + fp32) call NCHW,
NoFuse  NHWC + fp16 (=NCHW + fp16) call NHWC if mixed float is used.
Must have an activation layer (ReLU)


### Conv2D + FusedBatchBorm + Relu
This is only for inference on CPU. why training not? The backward pass of FusedBatchnorm needs output of Conv2D. No GPU implementation supported yet.
CPU only requires use_nhwc = True, BatchNorm has to be fused batchnorm in inference mode.

shouldn't use batch normtf.nn.batch_normalization, which is a serial of multiplication primitives.
```python
mean = tf.random.normal((k,))
variance = tf.random.normal((k,))
offset_np = np.random.random([k]).astype(np.float32)
offset = tf.Variable(offset_np)
scale_np = np.random.random([k]).astype(np.float32)
scale = tf.Variable(scale_np)

@tf.function
def fused_conv_bn_relu(x):
  with tf.device('/CPU:0'):
    y = tf.nn.conv2d(x, f, strides=(1, 1), padding='SAME',
                     data_format=x_format)
    y, _, _ = tf.compat.v1.nn.fused_batch_norm(
        y, scale, offset, mean, variance,
        data_format=x_format, is_training=False)
    y = tf.nn.relu(y)
  return y
```
The output:
```
I tensorflow/core/grappler/optimizers/remapper.cc:1252] Fuse Conv2D with BatchNorm and Relu: activation=Relu batch_norm=FusedBatchNormV3 conv2d=Conv2D
```
Similerly, using the high lever Keras APi like can trigger this this fusion as well. Don't forget to disable bias in conv2d layer and set the bn as False.
```python
conv2d_no_bias = tf.keras.layers.Conv2D(
                     filters=k, kernel_size=(r, s), padding='same',
                     data_format=x_format_keras, use_bias=False)
batch_norm = tf.keras.layers.BatchNormalization(axis=channel_axis)
# CPU only requires use_nhwc = True
@tf.function
def fused_conv_bn_relu_keras(x):
  with tf.device('/CPU:0'):
    y = conv2d_no_bias(x)
    y = batch_norm(y, training=False)
    y = relu(y)
  return y
```
### Conv2D + Squeeze + BiasAdd
This is only on CPU.
61 # CPU only requires use_nhwc = True, the output of conv happens to be 1 like# output (1,1,2,3), Then we squeeze the h and get 1,2,3 tensor for the bias add.
```python
@tf.function
def fused_conv_squeeze_bias(x):
  with tf.device('/CPU:0'):
    y = tf.nn.conv2d(x, f, strides=[1,1], padding='VALID',
                     data_format=x_format) 
    y = tf.squeeze(y, axis=1 if use_nhwc else 2) # output (1,2,3)
    y = tf.nn.bias_add(y, b, data_format=bias_format)
  return y
```
```
 I tensorflow/core/grappler/optimizers/remapper.cc:1166] Fuse Conv2D with Squeeze and BiasAdd:  bias_add=BiasAdd squeeze=Squeeze conv2d=Conv2D
```

There are other supported patterns like fused batch norm + side input are not covered in this post. In the future there might be other fusion patterns or the fused kernel being exposed as a single op if it is frequently used as a building block in newer deep learning models.

## Reference
* [TensorFlow graph optimization with Grappler](https://www.tensorflow.org/guide/graph_optimization)

