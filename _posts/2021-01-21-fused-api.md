---
layout: posts
title:  "Fused Operations in Tensorflow"
author: kaixi_hou
#search                   : true
#search_full_content      : true
#search_provider          : google
#comments: true
---
(Draft, Work in Progress)
## Introduction
The computations in deep learning models are usually represented by a graph.
Typically, operations in the graph are executed one by one, and each time we need
to perform memory read and write for their inputs and outputs respectively,
which could lead to performance issues since the offchip memory access is
oftentimes expensive. One way to improve that is to fuse the operations into a
big one to reduce such memory footprint. Apparently, it is impractical to replace
the whole graph into a monolithic operation; however, there are indeed many
patterns (a subgraph showing how operations are wired together) that are quite
common among various models, such as a _convolution_ followed by a _bias
addition_ or a _batch normalization_ followed by a _relu activation_. Tensorflow
remaps these patterns onto more efficient implementations via the Grappler
remapper optimizer. This post will discuss how the fusion is actually triggered
in TF with a focus on the convolution related patterns.

Below is the head part of the python script showing the configs we will use in
the following tests. At the very beginning, we turn on the remapper's logging to
show whether the target patterns are successfuly fused.

```python
import os
os.environ['TF_CPP_VMODULE'] = 'remapper=2'

import tensorflow as tf
import numpy as np

use_nhwc = True

N, H, W, C = 1, 2, 3, 3
k, r, s, c = 3, 2, 2, C

if use_nhwc:
  x_format = 'NHWC'
  x_format_keras = 'channels_last'
  bias_format = 'N...C'
  x_shape = (N, H, W, C)
  channel_axis = -1
else:
  x_format = 'NCHW'
  x_format_keras = 'channels_first'
  bias_format = 'NC...'
  x_shape = (N, C, H, W)
  channel_axis = 1
```

## Fused Convolution
### Conv2D + BiasAdd + Relu
How remapper do the optimization?
Search from the root of the pattern, sweep from the reversed order of the topological sort. For each node, if it is root of some pattern, it will go back to check other ops in the pattern and go  thought a series of checkings on the device, dtype, layout, etc. If everything looks good, a fused node will be replaced with the nodes in the pattern.
we use a common pattern to illustrate this process conv2d biasadd relu. We first
use the lower api to do the test as below, which we have to prepare the parameters filters and biases ourselves.
```python
f_np = np.random.random([r, s, c, k]).astype(np.float32)
f = tf.Variable(f_np)
b_np = np.random.random([k]).astype(np.float32)
b = tf.Variable(b_np)

@tf.function
def fused_conv_bias_relu(x):
  y = tf.nn.conv2d(x, f, strides=(1,1), padding='SAME',
                   data_format=x_format)
  y = tf.nn.bias_add(y, b, data_format=bias_format)
  y = tf.nn.relu(y)
  return y
```
We need to use the tf.function to make it as a graph and then graph optimizers including the remapper will be applied.
Then, we use the following code to call the above function
```python
inputs = tf.random.normal(x_shape)
outputs = fused_conv_bias_relu(inputs)
print(outputs)
```

Here, we are more interested in this line in the output (logging actived with TFMODEL), which shows the pattern is correctly found and fusion is done.
```
I tensorflow/core/grappler/optimizers/remapper.cc:1114]
  Fuse Conv2D with BiasAdd and Relu:
  activation=Relu bias_add=BiasAdd contraction=Conv2D
```
I also output the unoptimized graph and optimized graph from tensorboard (using SerializeAsString() output as pb file). the figures are adapted from the output of tensorboard:

<p align=center> Unfused graph </p>
![Title](/assets/posts_images/unfused.png)

<p align=center> Fused graph </p>
![Title](/assets/posts_images/fused.png)


We can see the relu node is the root and replaced with fused node. All the corresponding wires are changed as well that all the weights are to fused node and x to fused node as well.
Here is some note what requirements for current fusion to be triggered on GPU. Backend uses `cudnnConvolutionBiasActivationForward` from CUDNN for GPU.
To use the high level api like keras:

```python
conv2d = tf.keras.layers.Conv2D(
             filters=k, kernel_size=(r, s), padding='same',
             data_format=x_format_keras)
relu = tf.keras.layers.ReLU()

@tf.function
def fused_conv_bias_relu_keras(x):
  y = conv2d(x)
  y = relu(y)
  return y
```
Note, Support NHWC + fp32 (=NCHW + fp32) call NCHW,
NoFuse  NHWC + fp16 (=NCHW + fp16) call NHWC if mixed float is used.
Must have an activation layer (ReLU)


### Conv2D + FusedBatchBorm + Relu
This is only for inference on CPU. why training not? The backward pass of FusedBatchnorm needs output of Conv2D. No GPU implementation supported yet.
CPU only requires use_nhwc = True, BatchNorm has to be fused batchnorm in inference mode.

shouldn't use batch normtf.nn.batch_normalization, which is a serial of multiplication primitives.
```python
mean = tf.random.normal((k,))
variance = tf.random.normal((k,))
offset_np = np.random.random([k]).astype(np.float32)
offset = tf.Variable(offset_np)
scale_np = np.random.random([k]).astype(np.float32)
scale = tf.Variable(scale_np)

@tf.function
def fused_conv_bn_relu(x):
  with tf.device('/CPU:0'):
    y = tf.nn.conv2d(x, f, strides=(1, 1), padding='SAME',
                     data_format=x_format)
    y, _, _ = tf.compat.v1.nn.fused_batch_norm(
        y, scale, offset, mean, variance,
        data_format=x_format, is_training=False)
    y = tf.nn.relu(y)
  return y
```
The output:
```
I tensorflow/core/grappler/optimizers/remapper.cc:1252]
  Fuse Conv2D with BatchNorm and Relu:
  activation=Relu batch_norm=FusedBatchNormV3 conv2d=Conv2D
```
Similerly, using the high lever Keras APi like can trigger this this fusion as well. Don't forget to disable bias in conv2d layer and set the bn as False.
```python
conv2d_no_bias = tf.keras.layers.Conv2D(
                     filters=k, kernel_size=(r, s), padding='same',
                     data_format=x_format_keras, use_bias=False)
batch_norm = tf.keras.layers.BatchNormalization(axis=channel_axis)
# CPU only requires use_nhwc = True
@tf.function
def fused_conv_bn_relu_keras(x):
  with tf.device('/CPU:0'):
    y = conv2d_no_bias(x)
    y = batch_norm(y, training=False)
    y = relu(y)
  return y
```
### Conv2D + Squeeze + BiasAdd
This is only on CPU.
61 # CPU only requires use_nhwc = True, the output of conv happens to be 1 like# output (1,1,2,3), Then we squeeze the h and get 1,2,3 tensor for the bias add.
```python
@tf.function
def fused_conv_squeeze_bias(x):
  with tf.device('/CPU:0'):
    y = tf.nn.conv2d(x, f, strides=[1,1], padding='VALID',
                     data_format=x_format) 
    y = tf.squeeze(y, axis=1 if use_nhwc else 2) # output (1,2,3)
    y = tf.nn.bias_add(y, b, data_format=bias_format)
  return y
```
```
I tensorflow/core/grappler/optimizers/remapper.cc:1166]
  Fuse Conv2D with Squeeze and BiasAdd:
  bias_add=BiasAdd squeeze=Squeeze conv2d=Conv2D
```

There are other supported patterns like fused batch norm + side input are not covered in this post. In the future there might be other fusion patterns or the fused kernel being exposed as a single op if it is frequently used as a building block in newer deep learning models.

## Reference
* [TensorFlow graph optimization with Grappler](https://www.tensorflow.org/guide/graph_optimization)

